{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Done by : Adnane El Bouhali"
      ],
      "metadata": {
        "id": "biM-Oonl0Bib"
      },
      "id": "biM-Oonl0Bib"
    },
    {
      "cell_type": "markdown",
      "id": "bb9af7be",
      "metadata": {
        "id": "bb9af7be"
      },
      "source": [
        "# TP : Word Embeddings for Classification\n",
        "\n",
        "## Objectives:\n",
        "\n",
        "Explore the various way to represent textual data by applying them to a relatively small French classification dataset based on professionnal certification titles - **RNCP** - and evaluate how they perform on the classification task.\n",
        "1. Using what we have previously seen, pre-process the data: clean it, obtain an appropriate vocabulary.\n",
        "2. Obtain representations: any that will allow us to obtain a vector representation of each document is appropriate.\n",
        "    - Symbolic: **BoW, TF-IDF**\n",
        "    - Dense document representations: via **Topic Modeling: LSA, LDA**\n",
        "    - Dense word representations: **SVD-reduced PPMI, Word2vec, GloVe**\n",
        "        - For these, you will need to implement a **function aggregating word representations into document representations**\n",
        "3. Perform classification: we can make things simple and only use a **logistic regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4c7af15",
      "metadata": {
        "id": "e4c7af15"
      },
      "source": [
        "## Necessary dependancies\n",
        "\n",
        "We will need the following packages:\n",
        "- The Machine Learning API Scikit-learn : http://scikit-learn.org/stable/install.html\n",
        "- The Natural Language Toolkit : http://www.nltk.org/install.html\n",
        "- Gensim: https://radimrehurek.com/gensim/\n",
        "\n",
        "These are available with Anaconda: https://anaconda.org/anaconda/nltk and https://anaconda.org/anaconda/scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "079ef84c",
      "metadata": {
        "id": "079ef84c"
      },
      "outputs": [],
      "source": [
        "import os.path as op\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pprint\n",
        "import pandas as pd\n",
        "import gzip\n",
        "pp = pprint.PrettyPrinter(indent=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a147280",
      "metadata": {
        "id": "0a147280"
      },
      "source": [
        "## Loading data\n",
        "\n",
        "Let's load the data: take a first look."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b7a5e3e2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7a5e3e2",
        "outputId": "35dc60c5-1850-4663-9da7-67a81d2da729"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Categorie                                text_certifications\n",
            "0          1  Responsable de chantiers de bûcheronnage manue...\n",
            "1          1  Responsable de chantiers de bûcheronnage manue...\n",
            "2          1                                 Travaux forestiers\n",
            "3          1                                              Forêt\n",
            "4          1                                              Forêt\n"
          ]
        }
      ],
      "source": [
        "with open(\"rncp.csv\", encoding='utf-8') as f:\n",
        "    rncp = pd.read_csv(f, na_filter=False)\n",
        "\n",
        "print(rncp.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "2b21e6e3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b21e6e3",
        "outputId": "2cf7422c-e382-4423-f450-94f1c6e183b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Categorie' 'text_certifications']\n"
          ]
        }
      ],
      "source": [
        "print(rncp.columns.values)\n",
        "texts = rncp.loc[:,'text_certifications'].astype('str').tolist()\n",
        "labels = rncp.loc[:,'Categorie'].astype('str').tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "266fa04f",
      "metadata": {
        "id": "266fa04f"
      },
      "source": [
        "You can see that the first column is the category, the second the title of the certification. Let's get the category names for clarity:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9293f236",
      "metadata": {
        "id": "9293f236"
      },
      "outputs": [],
      "source": [
        "Categories = [\"1-environnement\",\n",
        "              \"2-defense\",\n",
        "              \"3-patrimoine\",\n",
        "              \"4-economie\",\n",
        "              \"5-recherche\",\n",
        "              \"6-nautisme\",\n",
        "              \"7-aeronautique\",\n",
        "              \"8-securite\",\n",
        "              \"9-multimedia\",\n",
        "              \"10-humanitaire\",\n",
        "              \"11-nucleaire\",\n",
        "              \"12-enfance\",\n",
        "              \"13-saisonnier\",\n",
        "              \"14-assistance\",\n",
        "              \"15-sport\",\n",
        "              \"16-ingenierie\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a2abb70a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2abb70a",
        "outputId": "0fcb2f61-57c9-4d22-aa06-316ac4f7f4b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  'Responsable de chantiers de bûcheronnage manuel et de débardage',\n",
            "   'Responsable de chantiers de bûcheronnage manuel et de sylviculture',\n",
            "   'Travaux forestiers',\n",
            "   'Forêt',\n",
            "   'Forêt',\n",
            "   'Responsable de chantiers forestiers',\n",
            "   'Diagnostic et taille des arbres',\n",
            "   'option Chef d’entreprise ou OHQ en travaux forestiers, spécialité '\n",
            "   'abattage-façonnage',\n",
            "   'option Chef d’entreprise ou OHQ en travaux forestiers, spécialité '\n",
            "   'débardage',\n",
            "   'Gestion et conduite de chantiers forestiers']\n"
          ]
        }
      ],
      "source": [
        "pp.pprint(texts[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "9599792e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9599792e",
        "outputId": "7c3a917a-20e8-44f9-93fc-3fb130774e71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of documents: 94312\n"
          ]
        }
      ],
      "source": [
        "# This number of documents may be high for some computers: we can select a fraction of them (here, one in k)\n",
        "# Use an even number to keep the same number of positive and negative reviews\n",
        "k = 1\n",
        "texts_reduced = texts[0::k]\n",
        "labels_reduced = labels[0::k]\n",
        "\n",
        "print('Number of documents:', len(texts_reduced))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fcd53e33",
      "metadata": {
        "id": "fcd53e33"
      },
      "source": [
        "Use the function ```train_test_split```from ```sklearn``` function to set aside test data that you will use during the lab. Make it one fifth of the data you have currently.\n",
        "\n",
        "<div class='alert alert-block alert-info'>\n",
        "            Code:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "c5be9da0",
      "metadata": {
        "id": "c5be9da0"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "texts_reduced, test_texts, labels_reduced, test_labels = train_test_split(texts_reduced, labels_reduced, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acbce6d4",
      "metadata": {
        "id": "acbce6d4"
      },
      "source": [
        "## 1 - Document Preprocessing\n",
        "\n",
        "You should use a pre-processing function you can apply to the raw text before any other processing (*i.e*, tokenization and obtaining representations). Some pre-processing can also be tied with the tokenization (*i.e*, removing stop words). Complete the following function, using the appropriate ```nltk``` tools.\n",
        "<div class='alert alert-block alert-info'>\n",
        "            Code:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "5e076e6e",
      "metadata": {
        "id": "5e076e6e"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "import string"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7632db6f",
      "metadata": {
        "id": "7632db6f"
      },
      "source": [
        "<div class='alert alert-block alert-info'>\n",
        "            Code:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "9b33d4d2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b33d4d2",
        "outputId": "239616af-6475-4718-a86c-150ecc3a0e9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['responsable', 'chantiers', 'bûcheronnage', 'manuel', 'débardage']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# Look at the data and apply the appropriate pre-processing\n",
        "# Download necessary NLTK datasets\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Define a preprocessing function\n",
        "def preprocess_text(text, language='french'):\n",
        "    # Tokenize text\n",
        "    tokens = word_tokenize(text, language=language)\n",
        "\n",
        "    # Convert to lower case\n",
        "    tokens = [word.lower() for word in tokens]\n",
        "\n",
        "    # Remove punctuation from each word\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    stripped = [word.translate(table) for word in tokens]\n",
        "\n",
        "    # Remove remaining tokens that are not alphabetic\n",
        "    words = [word for word in stripped if word.isalpha()]\n",
        "\n",
        "    # Filter out stop words\n",
        "    stop_words = set(stopwords.words(language))\n",
        "    words = [word for word in words if not word in stop_words]\n",
        "\n",
        "    # Optionally: Perform stemming (we'll skip lemmatization for simplicity)\n",
        "    # stemmer = SnowballStemmer(language)\n",
        "    # stemmed = [stemmer.stem(word) for word in words]\n",
        "\n",
        "    return words\n",
        "\n",
        "# Test the preprocessing function on a sample text\n",
        "sample_text = \"Responsable de chantiers de bûcheronnage manuel et de débardage\"\n",
        "preprocessed_sample = preprocess_text(sample_text)\n",
        "preprocessed_sample\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c4140f1",
      "metadata": {
        "id": "2c4140f1"
      },
      "source": [
        "Now that the data is cleaned, the first step we will follow is to pick a common vocabulary that we will use for every representations we obtain in this lab. **Use the code of the previous lab to create a vocabulary.**\n",
        "\n",
        "<div class='alert alert-block alert-info'>\n",
        "            Code:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "6910a7e4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6910a7e4",
        "outputId": "3118a388-c6fb-48ff-95e6-2f510dcc07cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 5953\n",
            "['a', 'aapapd', 'aaqcb', 'abattagefaçonnage', 'ac', 'accessibilité', 'accessoires', 'accompagnant', 'accompagnateur', 'accompagnateure', 'accompagnement', 'accompagné', 'accordeur', 'accordéon', 'accu', 'accueil', 'accueillant', 'accès', 'achard', 'achat', 'achats', 'achatvente', 'acheteur', 'acheteurs', 'acoustique', 'acoustiques', 'acquisition', 'acquisitions', 'acrobatique', 'acrobatiques', 'acse', 'acsyon', 'acteer', 'acteur', 'acteurs', 'actifs', 'action', 'actions', 'activite', 'activites', 'activité', 'activités', 'actuaire', 'actuariat', 'actuariel', 'actuarielle', 'actuelles', 'actuels', 'adaptation', 'adaptee', 'adapté', 'adaptée', 'adaptées', 'adaptés', 'additifs', 'additive', 'adhérents', 'adhésifs', 'adjoint', 'admin', 'administrateur', 'administrateurproducteur', 'administrateurrice', 'administratif', 'administratifs', 'administration', 'administrationgestion', 'administrationmaintenance', 'administrations', 'administrative', 'administratives', 'adminsitration', 'adolescence', 'adolescent', 'adolescents', 'adour', 'adret', 'adultes', 'advanced', 'ae', 'aeroteam', 'aes', 'affaire', 'affaires', 'affinitaires', 'affûteur', 'affûteurrégleur', 'affûteurrémouleur', 'afitp', 'africains', 'afrique', 'agapsc', 'agefi', 'agence', 'agencement', 'agenceur', 'agent', 'ages', 'agglomérations', 'agi']\n"
          ]
        }
      ],
      "source": [
        "# Assuming texts_reduced is your list of texts that you want to process\n",
        "preprocessed_texts = [preprocess_text(text) for text in texts_reduced]\n",
        "\n",
        "# Flatten the list of lists into a single list containing all tokens\n",
        "all_tokens = [token for sublist in preprocessed_texts for token in sublist]\n",
        "\n",
        "# Create a set of unique words to form the vocabulary\n",
        "vocabulary = set(all_tokens)\n",
        "\n",
        "print(f\"Vocabulary size: {len(vocabulary)}\")\n",
        "# Optionally, you might want to sort the vocabulary for consistency\n",
        "sorted_vocabulary = sorted(list(vocabulary))\n",
        "\n",
        "# If you want to look at some of the vocabulary words\n",
        "print(sorted_vocabulary[:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcb7810f",
      "metadata": {
        "id": "bcb7810f"
      },
      "source": [
        "What do you think is the **appropriate vocabulary size here** ? Would any further pre-processing make sense ? Motivate your answer.\n",
        "\n",
        "<div class='alert alert-block alert-warning'>\n",
        "            Question:</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e97ce437",
      "metadata": {
        "id": "e97ce437"
      },
      "source": [
        "Determining the appropriate vocabulary size and deciding on further preprocessing steps for a text classification task, such as classifying professional certification titles (RNCP dataset), requires balancing several factors:\n",
        "\n",
        "1. **Dataset and Project Objectives:** The nature of your dataset and the specific goals of your project play a crucial role. A complex, topic-diverse dataset or projects aiming to capture specialized terminology may benefit from a larger vocabulary.\n",
        "\n",
        "2. **Dimensionality vs. Information Loss:**\n",
        "   - A larger vocabulary increases the feature space's dimensionality, which can lead to longer processing times and potential overfitting issues.\n",
        "   - Excessively reducing the vocabulary size might result in significant information loss, undermining the model's ability to differentiate between classes effectively.\n",
        "\n",
        "3. **Computational Resources:** The available memory and processing power limit the feasible size of the vocabulary. A balance must be found between computational efficiency and model performance.\n",
        "\n",
        "4. **Further Pre-processing Considerations:**\n",
        "   - **Rare Words:** Removing infrequently appearing words can streamline the vocabulary without heavily impacting the content.\n",
        "   - **Stemming/Lemmatization:** These can reduce the vocabulary size by normalizing word variations to their root form, though the impact varies by language.\n",
        "   - **Domain-Specific Stopwords:** Identifying and removing common but uninformative words specific to the dataset's domain can improve model focus on relevant terms.\n",
        "\n",
        "### Decision Guidelines:\n",
        "- **Experimentation is key:** There's no one-size-fits-all answer. Experiment with different vocabulary sizes and preprocessing strategies, and evaluate their impact on model performance.\n",
        "- **Use Evaluation Metrics:** Monitor changes in accuracy, F1-score, and other relevant metrics to guide the refinement of your vocabulary and preprocessing steps.\n",
        "- **Incorporate Domain Expertise:** Leveraging knowledge specific to the domain can help identify non-informative words or phrases to be excluded from the analysis.\n",
        "\n",
        "For the RNCP dataset, starting with a comprehensive vocabulary and iteratively refining it based on performance and computational constraints is advisable. This approach, coupled with strategic preprocessing, can help create effective text representations for classification tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "260f7b7d",
      "metadata": {
        "id": "260f7b7d"
      },
      "source": [
        "## 2 - Symbolic text representations\n",
        "\n",
        "We can use the ```CountVectorizer``` class from scikit-learn to obtain the first set of representations:\n",
        "- Use the appropriate argument to get your own vocabulary\n",
        "- Fit the vectorizer on your training data, transform your test data\n",
        "- Create a ```LogisticRegression``` model and train it with these representations. Display the confusion matrix using functions from ```sklearn.metrics```\n",
        "\n",
        "Then, re-execute the same pipeline with the ```TfidfVectorizer```.\n",
        "\n",
        "<div class='alert alert-block alert-info'>\n",
        "            Code:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "1601c205",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1601c205",
        "outputId": "567719ba-d006-40e1-a8c9-38f99af06466"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix (BoW):\n",
            "[[ 492  102   82   17   18   17   29  382   18   66  833   75   54  118\n",
            "    22   79]\n",
            " [ 259  147    1   12    7   21   15   61    2   27  604   15   24   23\n",
            "     9   75]\n",
            " [ 158    5   79    0    1    1    0  215   18    1  166   11   11  103\n",
            "     5    1]\n",
            " [  19    6    0   48   42   48   24    0    0   22   66    8   24    0\n",
            "     0   22]\n",
            " [  29    2    1   46   53   50   24    0    2    5   33    1   27    2\n",
            "     6   11]\n",
            " [  36    9   10   42   39  121   44    6   10   33  110    7   21   21\n",
            "    27   32]\n",
            " [  42    4    0   34   36   54   36    1    7   25  332   13   51    2\n",
            "     0  130]\n",
            " [ 316   51   69    0    0    4    2  523    9    1  701   62   34  415\n",
            "     8  111]\n",
            " [  25    4   26    0    1    8    2   19   46    0   30    2   31   36\n",
            "    14    2]\n",
            " [ 115    8    1   25   17   15   27   24    1  164  239   94   19    7\n",
            "     0   38]\n",
            " [ 359  103   31    7    6   17   39  511    5   54 2095  102   31  154\n",
            "    19  331]\n",
            " [ 160    6    6    2    2   12    8  187    1   92  295   54   35   99\n",
            "     1   15]\n",
            " [ 172   19   15   14   21   19   20  202   34   13  208   59   73  114\n",
            "    18    7]\n",
            " [  96   12   52    0    2   14    0  554   14    2  358   37   17  280\n",
            "    25   66]\n",
            " [  62    1   29    0    0   20    0   61    7    0   55    3    4   21\n",
            "    43    0]\n",
            " [  36    6    0   14   10   16   42  107    0   36  742    4    2   19\n",
            "     0  364]]\n",
            "\n",
            "Classification Report (BoW):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.21      0.20      0.21      2404\n",
            "          10       0.30      0.11      0.16      1302\n",
            "          11       0.20      0.10      0.13       775\n",
            "          12       0.18      0.15      0.16       329\n",
            "          13       0.21      0.18      0.19       292\n",
            "          14       0.28      0.21      0.24       568\n",
            "          15       0.12      0.05      0.07       767\n",
            "          16       0.18      0.23      0.20      2306\n",
            "           2       0.26      0.19      0.22       246\n",
            "           3       0.30      0.21      0.25       794\n",
            "           4       0.31      0.54      0.39      3864\n",
            "           5       0.10      0.06      0.07       975\n",
            "           6       0.16      0.07      0.10      1008\n",
            "           7       0.20      0.18      0.19      1529\n",
            "           8       0.22      0.14      0.17       306\n",
            "           9       0.28      0.26      0.27      1398\n",
            "\n",
            "    accuracy                           0.24     18863\n",
            "   macro avg       0.22      0.18      0.19     18863\n",
            "weighted avg       0.23      0.24      0.22     18863\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Assuming texts_reduced and test_texts are your training and test sets, respectively\n",
        "# and labels_reduced and test_labels are the corresponding labels\n",
        "\n",
        "# Initialize CountVectorizer with your vocabulary\n",
        "vectorizer = CountVectorizer(vocabulary=sorted_vocabulary)  # Use your sorted_vocabulary\n",
        "\n",
        "# Fit the vectorizer on the training data and transform both training and test data\n",
        "X_train_counts = vectorizer.fit_transform(texts_reduced)\n",
        "X_test_counts = vectorizer.transform(test_texts)\n",
        "\n",
        "# Create and train a LogisticRegression model\n",
        "lr_model_counts = LogisticRegression(max_iter=1000)  # Increase max_iter if needed\n",
        "lr_model_counts.fit(X_train_counts, labels_reduced)\n",
        "\n",
        "# Predict on test data and display the confusion matrix\n",
        "predictions_counts = lr_model_counts.predict(X_test_counts)\n",
        "print(\"Confusion Matrix (BoW):\")\n",
        "print(confusion_matrix(test_labels, predictions_counts))\n",
        "print(\"\\nClassification Report (BoW):\")\n",
        "print(classification_report(test_labels, predictions_counts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "70c33e33",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70c33e33",
        "outputId": "3937a9a4-9e1d-4ed3-fe4b-154944fd4120"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix (TF-IDF):\n",
            "[[ 593   71   70   15   22   12   27  415   13   58  768   67   48  116\n",
            "    18   91]\n",
            " [ 287  151    0   15    6   16   16   66    2   29  562   13   20   24\n",
            "     7   88]\n",
            " [ 155    4   83    0    1    1    0  234   14    2  142   13    9  112\n",
            "     3    2]\n",
            " [  23    6    0   56   38   42   26    1    0   29   64    6   24    0\n",
            "     0   14]\n",
            " [  25    1    1   45   60   46   20    0    2    9   32    2   34    0\n",
            "     6    9]\n",
            " [  38    7    8   38   30  136   42    8    7   31  114    6   29   18\n",
            "    24   32]\n",
            " [  46    3    0   29   36   49   39    2    8   29  314   14   53    3\n",
            "     0  142]\n",
            " [ 333   48   59    0    1    3    1  624    5    0  650   57   29  390\n",
            "     5  101]\n",
            " [  31    3   24    0    0    6    0   19   39    0   34    1   32   40\n",
            "    15    2]\n",
            " [ 126    6    0   21   14   17   30   24    1  170  238   80   15   10\n",
            "     0   42]\n",
            " [ 382  101   30    5    4   17   39  570    2   54 2043  101   16  156\n",
            "    16  328]\n",
            " [ 185    5    4    3    0   12   12  209    0   85  266   59   26   96\n",
            "     0   13]\n",
            " [ 178   14   12   13   22   20   17  232   33   14  190   49   80  112\n",
            "    15    7]\n",
            " [  99   16   42    0    2   15    0  589   12    2  326   42    8  292\n",
            "    20   64]\n",
            " [  66    1   24    0    0   21    0   63    5    0   58    3    6   18\n",
            "    41    0]\n",
            " [  35    6    0   13    7   17   30  125    2   31  695    5    2   16\n",
            "     0  414]]\n",
            "\n",
            "Classification Report (TF-IDF):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.23      0.25      0.24      2404\n",
            "          10       0.34      0.12      0.17      1302\n",
            "          11       0.23      0.11      0.15       775\n",
            "          12       0.22      0.17      0.19       329\n",
            "          13       0.25      0.21      0.22       292\n",
            "          14       0.32      0.24      0.27       568\n",
            "          15       0.13      0.05      0.07       767\n",
            "          16       0.20      0.27      0.23      2306\n",
            "           2       0.27      0.16      0.20       246\n",
            "           3       0.31      0.21      0.25       794\n",
            "           4       0.31      0.53      0.39      3864\n",
            "           5       0.11      0.06      0.08       975\n",
            "           6       0.19      0.08      0.11      1008\n",
            "           7       0.21      0.19      0.20      1529\n",
            "           8       0.24      0.13      0.17       306\n",
            "           9       0.31      0.30      0.30      1398\n",
            "\n",
            "    accuracy                           0.26     18863\n",
            "   macro avg       0.24      0.19      0.20     18863\n",
            "weighted avg       0.25      0.26      0.24     18863\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize TfidfVectorizer with your vocabulary\n",
        "tfidf_vectorizer = TfidfVectorizer(vocabulary=sorted_vocabulary)  # Use your sorted_vocabulary\n",
        "\n",
        "# Fit the vectorizer on the training data and transform both training and test data\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(texts_reduced)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(test_texts)\n",
        "\n",
        "# Reuse the LogisticRegression model with TF-IDF representations\n",
        "lr_model_tfidf = LogisticRegression(max_iter=1000)  # Increase max_iter if needed\n",
        "lr_model_tfidf.fit(X_train_tfidf, labels_reduced)\n",
        "\n",
        "# Predict on test data and display the confusion matrix\n",
        "predictions_tfidf = lr_model_tfidf.predict(X_test_tfidf)\n",
        "print(\"Confusion Matrix (TF-IDF):\")\n",
        "print(confusion_matrix(test_labels, predictions_tfidf))\n",
        "print(\"\\nClassification Report (TF-IDF):\")\n",
        "print(classification_report(test_labels, predictions_tfidf))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ed387bc",
      "metadata": {
        "id": "0ed387bc"
      },
      "source": [
        "## 3 - Dense Representations from Topic Modeling\n",
        "\n",
        "Now, the goal is to re-use the bag-of-words representations we obtained earlier - but reduce their dimension through a **topic model**. Note that this allows to obtain reduced **document representations**, which we can again use directly to perform classification.\n",
        "- Do this with two models: ```TruncatedSVD``` and ```LatentDirichletAllocation```\n",
        "- Pick $300$ as the dimensionality of the latent representation (*i.e*, the number of topics)\n",
        "\n",
        "<div class='alert alert-block alert-info'>\n",
        "            Code:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "f4204cc9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4204cc9",
        "outputId": "56f9cec9-d5f9-4fe6-be31-83ef523910e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix (LSA):\n",
            "[[ 583   52   73   13   19   28   27  383   13   62  833   57   26  124\n",
            "    12   99]\n",
            " [ 257  121    1   13    7   20   21   65    3   25  614    8   17   30\n",
            "     8   92]\n",
            " [ 154    2   79    0    7    2    0  200    4    4  181    9    7  117\n",
            "     5    4]\n",
            " [  23    4    0   52   35   38   25    0    0   25   77    6   26    0\n",
            "     2   16]\n",
            " [  26    1    1   38   54   54    8    1    1   10   52    1   30    0\n",
            "     6    9]\n",
            " [  39    7    3   42   38  115   26   12    9   26  138    7   22   19\n",
            "    26   39]\n",
            " [  34    2    1   38   31   42   37    3    4   25  338   11   56    2\n",
            "     0  143]\n",
            " [ 326   43   53    1    0    6    0  680    5    5  684   49    9  342\n",
            "     7   96]\n",
            " [  46    3   16    0    4    8    1   13   25    0   50    0   33   29\n",
            "    14    4]\n",
            " [ 123    5    1   22   10   26   29   31    2  171  254   57   10   12\n",
            "     0   41]\n",
            " [ 327   74   35    6    4   26   42  542    0   59 2181   80   14  165\n",
            "    19  290]\n",
            " [ 188    6   10    3    2   10   18  198    3   56  275   79    8  106\n",
            "     0   13]\n",
            " [ 169   10   11   14   29   24    6  211   25   18  227   45   82  113\n",
            "    16    8]\n",
            " [ 121   20   45    1    2   10    0  515   11    6  369   32    5  310\n",
            "    22   60]\n",
            " [  56    2   21    1    6   13    0   68    3    0   64    4    5   12\n",
            "    51    0]\n",
            " [  35    2    0   11    2   22   34  138    0   34  662    5    5   17\n",
            "     0  431]]\n",
            "\n",
            "Classification Report (LSA):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.23      0.24      0.24      2404\n",
            "          10       0.34      0.09      0.15      1302\n",
            "          11       0.23      0.10      0.14       775\n",
            "          12       0.20      0.16      0.18       329\n",
            "          13       0.22      0.18      0.20       292\n",
            "          14       0.26      0.20      0.23       568\n",
            "          15       0.14      0.05      0.07       767\n",
            "          16       0.22      0.29      0.25      2306\n",
            "           2       0.23      0.10      0.14       246\n",
            "           3       0.33      0.22      0.26       794\n",
            "           4       0.31      0.56      0.40      3864\n",
            "           5       0.18      0.08      0.11       975\n",
            "           6       0.23      0.08      0.12      1008\n",
            "           7       0.22      0.20      0.21      1529\n",
            "           8       0.27      0.17      0.21       306\n",
            "           9       0.32      0.31      0.31      1398\n",
            "\n",
            "    accuracy                           0.27     18863\n",
            "   macro avg       0.25      0.19      0.20     18863\n",
            "weighted avg       0.26      0.27      0.24     18863\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "# Assuming X_train_counts is your BoW representation for the training data\n",
        "# and 300 is the desired number of topics\n",
        "\n",
        "# Initialize and fit TruncatedSVD\n",
        "svd_model = TruncatedSVD(n_components=300, random_state=42)\n",
        "lsa_transformed_train = svd_model.fit_transform(X_train_counts)\n",
        "\n",
        "# Optionally, normalize the output (helpful for some types of analysis)\n",
        "lsa_transformed_train = Normalizer(copy=False).fit_transform(lsa_transformed_train)\n",
        "\n",
        "# Transform the test data\n",
        "lsa_transformed_test = svd_model.transform(X_test_counts)\n",
        "lsa_transformed_test = Normalizer(copy=False).transform(lsa_transformed_test)\n",
        "\n",
        "# Train a logistic regression model on the LSA-transformed data\n",
        "lr_model_lsa = LogisticRegression(max_iter=1000)\n",
        "lr_model_lsa.fit(lsa_transformed_train, labels_reduced)\n",
        "\n",
        "# Predict and display metrics\n",
        "predictions_lsa = lr_model_lsa.predict(lsa_transformed_test)\n",
        "print(\"Confusion Matrix (LSA):\")\n",
        "print(confusion_matrix(test_labels, predictions_lsa))\n",
        "print(\"\\nClassification Report (LSA):\")\n",
        "print(classification_report(test_labels, predictions_lsa))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "d9a4eafa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9a4eafa",
        "outputId": "7193cb4e-e499-442a-b326-57a363d7dde6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix (LDA):\n",
            "[[ 606   36   40   11   15   11   16  409    2   52  953   35   38   95\n",
            "    15   70]\n",
            " [ 267   80    1   12    4   14   17   72    1   21  721    8   11   15\n",
            "     5   53]\n",
            " [ 165    2   43    0    3    1    0  242    4    3  200    5    8   86\n",
            "     9    4]\n",
            " [  37    6    0   38   25   17   18    1    0   23  103    6   32    0\n",
            "     1   22]\n",
            " [  40    1    1   29   26   20   11    1    0   13  105    0   30    6\n",
            "     3    6]\n",
            " [  85    7    1   34   15   38   21   18    0   28  207   12   26   23\n",
            "    23   30]\n",
            " [  50    4    0   33   19   26   37    9    3   26  387   14   45    8\n",
            "     1  105]\n",
            " [ 333   31   36    1    1    3    1  767    2    4  724   23   18  250\n",
            "     8  104]\n",
            " [  41    5   17    0    1    2    0   22   16    2   55    1   40   31\n",
            "    11    2]\n",
            " [ 146    6    0   12   10   11   23   36    0  139  307   43   10   11\n",
            "     0   40]\n",
            " [ 380   60   11    5    5   18   31  586    2   39 2282   49   19  119\n",
            "     8  250]\n",
            " [ 183   14    1    3    2    9    8  215    0   52  324   46   15   84\n",
            "     1   18]\n",
            " [ 178    2    2   17   11   16   10  260   16   11  281   27   74   82\n",
            "     8   13]\n",
            " [ 158   13   18    0    0    5    1  557    1    2  413   19   18  242\n",
            "    18   64]\n",
            " [  68    2   14    1    1    8    0   62    0    0   86    3    5   18\n",
            "    33    5]\n",
            " [  37    5    0   10    3   18   22  146    0   32  719    8    5   24\n",
            "     0  369]]\n",
            "\n",
            "Classification Report (LDA):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.22      0.25      0.23      2404\n",
            "          10       0.29      0.06      0.10      1302\n",
            "          11       0.23      0.06      0.09       775\n",
            "          12       0.18      0.12      0.14       329\n",
            "          13       0.18      0.09      0.12       292\n",
            "          14       0.18      0.07      0.10       568\n",
            "          15       0.17      0.05      0.08       767\n",
            "          16       0.23      0.33      0.27      2306\n",
            "           2       0.34      0.07      0.11       246\n",
            "           3       0.31      0.18      0.22       794\n",
            "           4       0.29      0.59      0.39      3864\n",
            "           5       0.15      0.05      0.07       975\n",
            "           6       0.19      0.07      0.11      1008\n",
            "           7       0.22      0.16      0.18      1529\n",
            "           8       0.23      0.11      0.15       306\n",
            "           9       0.32      0.26      0.29      1398\n",
            "\n",
            "    accuracy                           0.26     18863\n",
            "   macro avg       0.23      0.16      0.17     18863\n",
            "weighted avg       0.24      0.26      0.22     18863\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "# Initialize and fit LatentDirichletAllocation\n",
        "lda_model = LatentDirichletAllocation(n_components=300, random_state=42, learning_method='batch')\n",
        "lda_transformed_train = lda_model.fit_transform(X_train_counts)\n",
        "\n",
        "# Transform the test data\n",
        "lda_transformed_test = lda_model.transform(X_test_counts)\n",
        "\n",
        "# Train a logistic regression model on the LDA-transformed data\n",
        "lr_model_lda = LogisticRegression(max_iter=1000)\n",
        "lr_model_lda.fit(lda_transformed_train, labels_reduced)\n",
        "\n",
        "# Predict and display metrics\n",
        "predictions_lda = lr_model_lda.predict(lda_transformed_test)\n",
        "print(\"Confusion Matrix (LDA):\")\n",
        "print(confusion_matrix(test_labels, predictions_lda))\n",
        "print(\"\\nClassification Report (LDA):\")\n",
        "print(classification_report(test_labels, predictions_lda))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4893c948",
      "metadata": {
        "id": "4893c948"
      },
      "source": [
        "<div class='alert alert-block alert-warning'>\n",
        "            Question:</div>\n",
        "            \n",
        "We picked $300$ as number of topics. What would be the procedure to follow if we wanted to choose this hyperparameter through the data ?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "730b7614",
      "metadata": {
        "id": "730b7614"
      },
      "source": [
        "To determine the optimal number of topics for topic modeling in text classification tasks, a structured, data-driven approach is recommended, blending quantitative evaluation with qualitative insights:\n",
        "\n",
        "1. **Initial Setup:** Begin by selecting a range of potential numbers of topics. This range should be informed by your dataset's characteristics and the computational resources at your disposal.\n",
        "\n",
        "2. **Quantitative Evaluation:** Use metrics to quantitatively evaluate the performance of your topic models across the range of topic numbers. While coherence scores and perplexity are standard metrics in topic modeling, you might rely on the performance of a downstream classification task, such as accuracy, as a proxy for topic model quality in environments where these standard metrics are not readily available.\n",
        "\n",
        "3. **Cross-Validation:** Implement cross-validation to ensure the stability and robustness of your chosen hyperparameter across different data subsets. This step helps in verifying that the selected number of topics is not overfitted to a specific partition of your dataset.\n",
        "\n",
        "4. **Iterative Testing:** Conduct iterative tests over your defined range of topic numbers, assessing each configuration's performance based on your chosen evaluation metric. This could be done through grid search techniques or more sophisticated optimization methods.\n",
        "\n",
        "5. **Qualitative Evaluation:** Complement quantitative assessments with qualitative evaluations of the topics generated. Inspect the coherence, distinctiveness, and relevance of topics manually to ensure they align with analytical goals and exhibit meaningful patterns.\n",
        "\n",
        "6. **Optimization and Final Selection:** Identify the optimal number of topics as the one that provides the best balance between quantitative performance (e.g., classification accuracy) and qualitative coherence. This optimal point is where the topics are not too broad to be meaningless nor too fine-grained to be overly specific.\n",
        "\n",
        "Throughout this process, the goal is to find a sweet spot where the topics are meaningful and contribute positively to the performance of downstream tasks like classification, balancing between the granularity of topics and the manageability of model complexity and interpretability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "446c105a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "446c105a",
        "outputId": "f7191bf9-e855-4c45-98c3-1b8a7963ecb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num Topics: 50, Accuracy: 0.23654773895986853\n",
            "Num Topics: 100, Accuracy: 0.24715050628213964\n",
            "Num Topics: 150, Accuracy: 0.25271695912633196\n",
            "Num Topics: 200, Accuracy: 0.25451942957111806\n",
            "Num Topics: 250, Accuracy: 0.253618194348725\n",
            "Num Topics: 300, Accuracy: 0.25642792768912687\n",
            "Num Topics: 350, Accuracy: 0.25785930127763346\n",
            "Optimal number of topics: 350\n"
          ]
        }
      ],
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Example range of topics to explore\n",
        "num_topics_range = range(50, 351, 50)  # From 50 to 350, stepping by 50\n",
        "performance_scores = []\n",
        "\n",
        "for num_topics in num_topics_range:\n",
        "    # Apply TruncatedSVD to reduce dimensions\n",
        "    svd_model = TruncatedSVD(n_components=num_topics, random_state=42)\n",
        "    X_train_reduced = svd_model.fit_transform(X_train_counts)\n",
        "    X_test_reduced = svd_model.transform(X_test_counts)\n",
        "\n",
        "    # Train logistic regression on the reduced dataset\n",
        "    lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "    lr_model.fit(X_train_reduced, labels_reduced)\n",
        "    predictions = lr_model.predict(X_test_reduced)\n",
        "\n",
        "    # Evaluate performance\n",
        "    score = accuracy_score(test_labels, predictions)\n",
        "    performance_scores.append(score)\n",
        "    print(f\"Num Topics: {num_topics}, Accuracy: {score}\")\n",
        "\n",
        "# Identify the number of topics with the best performance\n",
        "optimal_num_topics = num_topics_range[np.argmax(performance_scores)]\n",
        "print(\"Optimal number of topics:\", optimal_num_topics)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60f74d75",
      "metadata": {
        "id": "60f74d75"
      },
      "source": [
        "## 4 - Dense Count-based Representations\n",
        "\n",
        "The following function allows to obtain very large-dimensional vectors for **words**. We will now follow a different procedure:\n",
        "- Step 1: Obtain the co-occurence matrix, based on the vocabulary, giving you a vector by word in the vocabulary.\n",
        "- Step 2: Apply an SVD to obtain **word embeddings** of dimension $300$, for each word in the vocabulary.\n",
        "- Step 3: Obtain document representations by aggregating embeddings associated to each word in the document.\n",
        "- Step 4: Train a classifier on the (document representations, label) pairs.\n",
        "\n",
        "Some instructions:\n",
        "- In step 1, use the ```co_occurence_matrix``` function, which you need to complete.\n",
        "- In step 2, use ```TruncatedSVD```to obtain word representations of dimension $300$ from the output of the ```co_occurence_matrix``` function.\n",
        "- In step 3, use the ```sentence_representations``` function, which you will need to complete.\n",
        "- In step 4, put the pipeline together by obtaining document representations for both training and testing data. Careful: the word embeddings must come from the *training data co-occurence matrix* only.\n",
        "\n",
        "Lastly, add a **Step 1b**: transform the co-occurence matrix into the PPMI matrix, and compare the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "05ed10f4",
      "metadata": {
        "id": "05ed10f4"
      },
      "outputs": [],
      "source": [
        "def co_occurence_matrix(corpus, vocabulary, window=0):\n",
        "    \"\"\"\n",
        "    Params:\n",
        "        corpus (list of list of strings): corpus of sentences\n",
        "        vocabulary (dictionary): word to index mapping for the vocabulary\n",
        "        window (int): size of the context window; when 0, the context is the whole sentence\n",
        "    Returns:\n",
        "        matrix (np.array of size (len(vocabulary), len(vocabulary))): the co-oc matrix, using the same ordering as the vocabulary given in input\n",
        "    \"\"\"\n",
        "    vocab_size = len(vocabulary)\n",
        "    M = np.zeros((vocab_size, vocab_size))\n",
        "    for sent in corpus:\n",
        "        # Convert sentence words to indexes, based on the vocabulary\n",
        "        sent_idx = [vocabulary[word] for word in sent if word in vocabulary]\n",
        "\n",
        "        # Iterate through each word in the sentence\n",
        "        for i, word_idx in enumerate(sent_idx):\n",
        "            # Determine context based on window size\n",
        "            if window > 0:\n",
        "                # Limited context window\n",
        "                start = max(i - window, 0)\n",
        "                end = min(i + window + 1, len(sent_idx))\n",
        "            else:\n",
        "                # Whole sentence as context\n",
        "                start, end = 0, len(sent_idx)\n",
        "\n",
        "            # Update the co-occurrence matrix for words within the context\n",
        "            for j in range(start, end):\n",
        "                if i != j:  # Skip the word itself\n",
        "                    context_word_idx = sent_idx[j]\n",
        "                    M[word_idx, context_word_idx] += 1\n",
        "                    # The matrix is symmetric, but we fill it in both directions anyway\n",
        "                    M[context_word_idx, word_idx] += 1\n",
        "    return M\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7be497b3",
      "metadata": {
        "id": "7be497b3"
      },
      "source": [
        "<div class='alert alert-block alert-info'>\n",
        "            Code:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "e843cc8b",
      "metadata": {
        "id": "e843cc8b"
      },
      "outputs": [],
      "source": [
        "# Convert your vocabulary dictionary to a word:index format\n",
        "word_to_index = {word: i for i, word in enumerate(sorted(vocabulary))}\n",
        "\n",
        "# Obtain the co-occurrence matrix\n",
        "co_occ_matrix = co_occurence_matrix(preprocessed_texts, word_to_index)\n",
        "\n",
        "# Function to transform co-occurrence matrix into PPMI\n",
        "def compute_ppmi(co_occ_matrix):\n",
        "    total_count = np.sum(co_occ_matrix)\n",
        "    sum_over_rows = np.sum(co_occ_matrix, axis=1)\n",
        "    sum_over_cols = np.sum(co_occ_matrix, axis=0)\n",
        "    expected = np.outer(sum_over_rows, sum_over_cols) / total_count\n",
        "    with np.errstate(divide='ignore', invalid='ignore'):\n",
        "        ppmi_matrix = np.log2(co_occ_matrix * total_count / (expected + 1e-8))\n",
        "        ppmi_matrix[np.isinf(ppmi_matrix) | np.isnan(ppmi_matrix)] = 0\n",
        "        ppmi_matrix = np.maximum(ppmi_matrix, 0)\n",
        "    return ppmi_matrix\n",
        "\n",
        "\n",
        "# Optionally transform the co-occurrence matrix into PPMI\n",
        "ppmi_matrix = compute_ppmi(co_occ_matrix)\n",
        "\n",
        "# Apply TruncatedSVD to reduce dimensions\n",
        "svd = TruncatedSVD(n_components=300, random_state=42)\n",
        "word_embeddings = svd.fit_transform(ppmi_matrix)  # Use co_occ_matrix directly if not using PPMI\n",
        "\n",
        "# 'word_embeddings' now contains the 300-dimensional embeddings for each word in your vocabulary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b312da12",
      "metadata": {
        "id": "b312da12"
      },
      "source": [
        "<div class='alert alert-block alert-info'>\n",
        "            Code:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "e413841b",
      "metadata": {
        "id": "e413841b"
      },
      "outputs": [],
      "source": [
        "def sentence_representations(texts, vocabulary, embeddings, np_func=np.mean):\n",
        "    \"\"\"\n",
        "    Represent the sentences as a combination of the vector of its words.\n",
        "    Parameters\n",
        "    ----------\n",
        "    texts : a list of sentences\n",
        "    vocabulary : dict\n",
        "        From words to indexes of vector.\n",
        "    embeddings : Matrix containing word representations\n",
        "    np_func : function (default: np.sum)\n",
        "        A numpy matrix operation that can be applied columnwise,\n",
        "        like `np.mean`, `np.sum`, or `np.prod`.\n",
        "    Returns\n",
        "    -------\n",
        "    np.array, dimension `(len(texts), embeddings.shape[1])`\n",
        "    \"\"\"\n",
        "    representations = []\n",
        "    for text in texts:\n",
        "        # Retrieve indexes of words in the sentence from the vocabulary\n",
        "        indexes = [vocabulary[word] for word in text if word in vocabulary]\n",
        "\n",
        "        if not indexes:\n",
        "            # Handle sentences with no known vocabulary words\n",
        "            sent_rep = np.zeros(embeddings.shape[1])\n",
        "        else:\n",
        "            # Retrieve embeddings for these words\n",
        "            word_embeddings = embeddings[indexes]\n",
        "\n",
        "            # Aggregate embeddings of words in the sentence using np_func\n",
        "            sent_rep = np_func(word_embeddings, axis=0)\n",
        "\n",
        "        representations.append(sent_rep)\n",
        "\n",
        "    representations = np.array(representations)\n",
        "    return representations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2462a0a2",
      "metadata": {
        "id": "2462a0a2"
      },
      "source": [
        "<div class='alert alert-block alert-info'>\n",
        "            Code:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "a5256e11",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5256e11",
        "outputId": "52ea039a-ecb6-43bd-ac9f-ce6b4aeece5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.27328632773153794\n",
            "Confusion Matrix:\n",
            " [[ 532   70   67   20   23   25   29  411   17   71  795   65   46  125\n",
            "    15   93]\n",
            " [ 232  137    4   17    5   21   21   81    4   46  594   11   22   16\n",
            "     6   85]\n",
            " [ 115    3   84    0    4    2    0  238    8    5  156   17   10  124\n",
            "     7    2]\n",
            " [  18    3    0   55   43   40   36    0    0   33   56    7   18    0\n",
            "     1   19]\n",
            " [  23    1    1   38   72   38   25    0    2   13   33    2   24    0\n",
            "     5   15]\n",
            " [  37    8   10   39   58  115   29   10    7   29  108   10   21   17\n",
            "    30   40]\n",
            " [  30    4    1   32   48   40   52    2   11   38  312   15   39    4\n",
            "     0  139]\n",
            " [ 302   47   50    1    1    9    0  773    6    5  602   59   31  313\n",
            "     4  103]\n",
            " [  32    6   23    0    2    5    2   15   46    0   42    0   22   34\n",
            "    16    1]\n",
            " [ 109    5    1   27   14   30   30   29    2  189  227   60   17   15\n",
            "     1   38]\n",
            " [ 289   87   28   13    4   19   40  619    2   57 2152   96   23  143\n",
            "    17  275]\n",
            " [ 152    4   10    5    1   13    9  221    4   73  260   87   28   96\n",
            "     0   12]\n",
            " [ 161   18   10   15   28   20   12  226   39   17  203   50   80  110\n",
            "    14    5]\n",
            " [ 101   16   46    0    4   12    0  557   13    6  329   49   12  304\n",
            "    21   59]\n",
            " [  53    6   15    1    6    6    0   75    6    0   54    7    8   23\n",
            "    46    0]\n",
            " [  27    3    1   18    5   22   37  155    0   36  633    8    4   18\n",
            "     0  431]]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           1       0.24      0.22      0.23      2404\n",
            "          10       0.33      0.11      0.16      1302\n",
            "          11       0.24      0.11      0.15       775\n",
            "          12       0.20      0.17      0.18       329\n",
            "          13       0.23      0.25      0.24       292\n",
            "          14       0.28      0.20      0.23       568\n",
            "          15       0.16      0.07      0.10       767\n",
            "          16       0.23      0.34      0.27      2306\n",
            "           2       0.28      0.19      0.22       246\n",
            "           3       0.31      0.24      0.27       794\n",
            "           4       0.33      0.56      0.41      3864\n",
            "           5       0.16      0.09      0.11       975\n",
            "           6       0.20      0.08      0.11      1008\n",
            "           7       0.23      0.20      0.21      1529\n",
            "           8       0.25      0.15      0.19       306\n",
            "           9       0.33      0.31      0.32      1398\n",
            "\n",
            "    accuracy                           0.27     18863\n",
            "   macro avg       0.25      0.20      0.21     18863\n",
            "weighted avg       0.26      0.27      0.25     18863\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Obtain Document Representations\n",
        "# Convert your training and test texts into their vector representations\n",
        "# First, preprocess the texts to tokenize them, similar to how 'preprocessed_texts' was obtained\n",
        "preprocessed_train_texts = [preprocess_text(text) for text in texts_reduced]\n",
        "preprocessed_test_texts = [preprocess_text(text) for text in test_texts]\n",
        "\n",
        "# Now, use the sentence representations function\n",
        "X_train = sentence_representations(preprocessed_train_texts, word_to_index, word_embeddings, np_func=np.mean)\n",
        "X_test = sentence_representations(preprocessed_test_texts, word_to_index, word_embeddings, np_func=np.mean)\n",
        "\n",
        "# Step 2: Apply the Classifier\n",
        "# Initialize the logistic regression classifier\n",
        "lr_classifier = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "# Train the classifier on the training data\n",
        "lr_classifier.fit(X_train, labels_reduced)\n",
        "\n",
        "# Predict the labels for the test set\n",
        "predictions = lr_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier\n",
        "accuracy = accuracy_score(test_labels, predictions)\n",
        "conf_matrix = confusion_matrix(test_labels, predictions)\n",
        "class_report = classification_report(test_labels, predictions)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "print(\"Classification Report:\\n\", class_report)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a9dbec5",
      "metadata": {
        "id": "7a9dbec5"
      },
      "source": [
        "## 5 - Dense Prediction-based Representations\n",
        "\n",
        "We will now use word embeddings from ```Word2Vec```: which we will train ourselves\n",
        "\n",
        "We will use the ```gensim``` library for its implementation of word2vec in python. Since we want to keep the same vocabulary as before: we'll first create the model, then re-use the vocabulary we generated above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "80339bee",
      "metadata": {
        "id": "80339bee"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b48e7ec",
      "metadata": {
        "id": "0b48e7ec"
      },
      "source": [
        "<div class='alert alert-block alert-info'>\n",
        "            Code:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "895599f4",
      "metadata": {
        "id": "895599f4"
      },
      "outputs": [],
      "source": [
        "# The model is to be trained with a list of tokenized sentences, containing the full training dataset.\n",
        "# Preprocess your texts\n",
        "preprocessed_texts = [preprocess_text(text) for text in texts_reduced]\n",
        "\n",
        "# Initialize the Word2Vec model\n",
        "model = Word2Vec(vector_size=300, window=5, min_count=1)\n",
        "\n",
        "# Build the vocabulary from your preprocessed texts\n",
        "model.build_vocab(preprocessed_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "157ba2c6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "157ba2c6",
        "outputId": "373d8fa6-35c3-4da0-c9ff-93bdb97ce10e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11907371, 16946040)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "# Train the model on the preprocessed texts\n",
        "model.train(preprocessed_texts, total_examples=len(preprocessed_texts), epochs=30, report_delay=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb5e1daa",
      "metadata": {
        "id": "cb5e1daa"
      },
      "source": [
        "Then, we can re-use the ```sentence_representations```function like before to obtain document representations, and apply classification.\n",
        "<div class='alert alert-block alert-info'>\n",
        "            Code:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "66123a9b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66123a9b",
        "outputId": "57e84e10-3e2a-4c9a-bcb2-294fc6b022c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.27169591263319726\n",
            "Confusion Matrix:\n",
            " [[ 587   41   63   15   21   31   28  394   17   74  803   52   41  130\n",
            "    16   91]\n",
            " [ 267  108    0   14    9   27   20   72    3   41  615    8   13   20\n",
            "     7   78]\n",
            " [ 133    3   76    0    1   10    0  235   14    4  149   10   14  115\n",
            "     8    3]\n",
            " [  20    3    0   58   43   34   25    4    0   31   62    5   21    0\n",
            "     1   22]\n",
            " [  22    1    1   37   78   42   13    1    4   16   40    1   26    1\n",
            "     3    6]\n",
            " [  47    6    5   39   54  120   25   13   15   34  112    8   18   19\n",
            "    23   30]\n",
            " [  37    2    0   34   45   43   48    3    8   35  308   13   37    4\n",
            "     0  150]\n",
            " [ 313   35   49    0    1    8    0  766    6    4  643   45   25  315\n",
            "     7   89]\n",
            " [  36    3   22    0    3   11    1   13   35    1   41    1   30   32\n",
            "    12    5]\n",
            " [ 112    2    0   22   13   25   28   26    3  200  245   49   17   18\n",
            "     1   33]\n",
            " [ 327   58   32    5    5   37   39  591    1   56 2154   78   28  151\n",
            "    18  284]\n",
            " [ 161    3    7    2    3    6   12  214    3   77  269   77   20  108\n",
            "     0   13]\n",
            " [ 158    7    9   15   31   21    9  204   43   20  224   52   78  120\n",
            "    12    5]\n",
            " [  97   17   48    0    2   13    1  567   16    7  357   39   19  284\n",
            "    19   43]\n",
            " [  53    5   14    1    3   22    0   76    8    1   51    5    2   19\n",
            "    46    0]\n",
            " [  24    4    0   13    6   24   32  153    1   43  657    6    5   20\n",
            "     0  410]]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           1       0.25      0.24      0.24      2404\n",
            "          10       0.36      0.08      0.13      1302\n",
            "          11       0.23      0.10      0.14       775\n",
            "          12       0.23      0.18      0.20       329\n",
            "          13       0.25      0.27      0.26       292\n",
            "          14       0.25      0.21      0.23       568\n",
            "          15       0.17      0.06      0.09       767\n",
            "          16       0.23      0.33      0.27      2306\n",
            "           2       0.20      0.14      0.17       246\n",
            "           3       0.31      0.25      0.28       794\n",
            "           4       0.32      0.56      0.41      3864\n",
            "           5       0.17      0.08      0.11       975\n",
            "           6       0.20      0.08      0.11      1008\n",
            "           7       0.21      0.19      0.20      1529\n",
            "           8       0.27      0.15      0.19       306\n",
            "           9       0.32      0.29      0.31      1398\n",
            "\n",
            "    accuracy                           0.27     18863\n",
            "   macro avg       0.25      0.20      0.21     18863\n",
            "weighted avg       0.26      0.27      0.25     18863\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Update the vocabulary and embeddings from the trained Word2Vec model\n",
        "vocabulary = {word: idx for idx, word in enumerate(model.wv.index_to_key)}\n",
        "embeddings = np.vstack([model.wv[word] for word in model.wv.index_to_key])\n",
        "\n",
        "# Generate document vectors for training and testing sets\n",
        "X_train_vec = sentence_representations(preprocessed_train_texts, vocabulary, embeddings)\n",
        "X_test_vec = sentence_representations(preprocessed_test_texts, vocabulary, embeddings)\n",
        "\n",
        "# Train the classifier on the document vectors\n",
        "lr_classifier = LogisticRegression(max_iter=1000, random_state=42)\n",
        "lr_classifier.fit(X_train_vec, labels_reduced)\n",
        "\n",
        "# Predict the labels for the test set\n",
        "predictions_vec = lr_classifier.predict(X_test_vec)\n",
        "\n",
        "# Evaluate the classifier\n",
        "accuracy_vec = accuracy_score(test_labels, predictions_vec)\n",
        "conf_matrix_vec = confusion_matrix(test_labels, predictions_vec)\n",
        "class_report_vec = classification_report(test_labels, predictions_vec)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_vec)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix_vec)\n",
        "print(\"Classification Report:\\n\", class_report_vec)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b21842b8",
      "metadata": {
        "id": "b21842b8"
      },
      "source": [
        "<div class='alert alert-block alert-warning'>\n",
        "            Question:</div>\n",
        "            \n",
        "Comment on the results. What is the big issue with the dataset that using embeddings did not solve ?\n",
        "**Given this type of data**, what would you propose if you needed solve this task (i.e, reach a reasonnable performance) in an industrial context ?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The performance metrics indicate challenges with the dataset and model, evidenced by a relatively low accuracy of around 27.2% and varied performance across different classes. This situation hints at underlying issues such as class imbalance, contextual ambiguity, and possibly noisy data, which the use of embeddings alone did not adequately address.\n",
        "\n",
        "To tackle these challenges, especially in an industrial context where achieving reasonable performance is critical, a multifaceted approach is recommended:\n",
        "\n",
        "1. **Address Class Imbalance**: Implement resampling techniques or cost-sensitive training to balance the influence of different classes on the training process.\n",
        "\n",
        "2. **Enhance Model Capability**: Transition to more advanced NLP models like BERT, GPT, or RoBERTa that are better at capturing contextual nuances and have shown superior performance in a wide range of NLP tasks. Consider ensemble methods to leverage the strengths of multiple models.\n",
        "\n",
        "3. **Improve Data Quality**: Engage in more sophisticated preprocessing and data augmentation strategies to clean the data further and increase its diversity, helping the model to generalize better.\n",
        "\n",
        "4. **Experiment and Evaluate Thoroughly**: Employ cross-validation for more reliable performance evaluation and engage in systematic hyperparameter tuning to find the optimal model configuration.\n",
        "\n",
        "Implementing these strategies involves balancing computational resources and model complexity, with a keen eye on the dataset's specific characteristics. In an industrial setting, it's also crucial to establish a continuous evaluation loop, where the model's real-world performance is regularly monitored, and the model is updated or retrained as needed to adapt to new data and evolving requirements."
      ],
      "metadata": {
        "id": "xlNhNWqT0jmw"
      },
      "id": "xlNhNWqT0jmw"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}